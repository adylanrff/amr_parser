{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "import random\n",
    "import penman\n",
    "import pickle \n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.tree import plot_tree, export_graphviz\n",
    "from sklearn.metrics import confusion_matrix , f1_score, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from IPython.display import Image  \n",
    "from tqdm import tqdm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "RANDOM_STATE = 13516013\n",
    "random.seed(RANDOM_STATE)\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"data/raw/edge_prediction/dependency_parser_features_labeled.csv\"\n",
    "data_path = \"data/raw/edge_prediction/dependency_parser_features_labeled_no_unk.csv\"\n",
    "test_data_path = \"data/raw/edge_prediction/test_dependency_parser_features_labeled.csv\"\n",
    "amr_data_path = \"data/raw/edge_prediction/amr.csv\"\n",
    "amr_test_path = \"data/raw/edge_prediction/amr.test.csv\"\n",
    "\n",
    "amr_val_prediction_path = 'data/test/val'\n",
    "amr_test_prediction_path = 'data/test/test'\n",
    "\n",
    "\n",
    "feature_filter_type = ['positional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/raw/edge_prediction/dependency_parser_features_labeled_no_unk.csv does not exist: 'data/raw/edge_prediction/dependency_parser_features_labeled_no_unk.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0cc876ca6744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdependency_features_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mamr_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamr_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_amr_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamr_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File data/raw/edge_prediction/dependency_parser_features_labeled_no_unk.csv does not exist: 'data/raw/edge_prediction/dependency_parser_features_labeled_no_unk.csv'"
     ]
    }
   ],
   "source": [
    "dependency_features_df = pd.read_csv(data_path)\n",
    "amr_dataset = pd.read_csv(amr_data_path)\n",
    "test_amr_dataset = pd.read_csv(amr_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(dataset, filter_type='all'):\n",
    "    lexical_features = ['parent', 'child']\n",
    "    positional_features = ['parent_position', 'child_position']\n",
    "    structural_features = ['parent_pos', 'child_pos']\n",
    "    syntactic_features = ['dependency_role']\n",
    "    ner_features = ['parent_ner', 'child_ner']\n",
    "    \n",
    "    selected_dataset = dataset\n",
    "    \n",
    "    if filter_type != 'all':\n",
    "        if 'lexical' in filter_type:\n",
    "            selected_dataset = selected_dataset.drop(lexical_features, axis=1)\n",
    "        if 'positional' in filter_type:\n",
    "            selected_dataset = selected_dataset.drop(positional_features, axis=1)\n",
    "        if 'syntactic' in filter_type:\n",
    "            selected_dataset = selected_dataset.drop(syntactic_features+structural_features, axis=1)\n",
    "        if 'ner' in filter_type:\n",
    "            selected_dataset = selected_dataset.drop(ner_features, axis=1)\n",
    "        \n",
    "    return selected_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_dependency_parser_feature_k_fold(dataset, n_split, shuffle=False):\n",
    "    min_sentence_id = dataset.min()['sentence_id']\n",
    "    max_sentence_id = dataset.max()['sentence_id']\n",
    "    sentence_num = max_sentence_id\n",
    "    \n",
    "    sentence_ids = np.arange(min_sentence_id, max_sentence_id+1)\n",
    "    if (shuffle):\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        np.random.shuffle(sentence_ids)\n",
    "    \n",
    "    cur_sentence_id = min_sentence_id\n",
    "    split_size = sentence_num // n_split\n",
    "    \n",
    "    indexes = []\n",
    "    while (cur_sentence_id < max_sentence_id):\n",
    "        stop_sentence_id = min(cur_sentence_id + split_size-1, sentence_num)\n",
    "        test_sentence_ids = sentence_ids[cur_sentence_id:stop_sentence_id+1]\n",
    "        \n",
    "        test_condition = dataset.sentence_id.isin(test_sentence_ids)\n",
    "        train_condition = ~test_condition \n",
    "        \n",
    "        indexes.append((dataset[train_condition].index, dataset[test_condition].index))\n",
    "        cur_sentence_id = stop_sentence_id\n",
    "    \n",
    "    return indexes\n",
    "    \n",
    "def split_sentence(dataset, split):\n",
    "    random.seed(RANDOM_STATE)\n",
    "    sentence_num = dataset.max()['sentence_id']\n",
    "    train_length = round(sentence_num*(100-(split*100))/100)\n",
    "    sentence_indexes = random.sample(range(1, sentence_num+1), int(train_length))    \n",
    "    data_train, data_val = dataset[dataset.sentence_id.isin(sentence_indexes)], dataset[~dataset.sentence_id.isin(sentence_indexes)]\n",
    "    label = ['label']\n",
    "    return data_train.drop(label, axis=1), data_val.drop(label, axis=1), data_train[label], data_val[label]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Val AMR data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_amr_data(amr_data, filepath):\n",
    "    with open(filepath + '.txt', 'w+', encoding='utf-8') as f:\n",
    "        for idx, amr_graph in amr_data.values:\n",
    "            f.write(amr_graph)\n",
    "            f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(dependency_features_df['label'])\n",
    "labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "filtered_dependency_features_df = feature_selection(dependency_features_df, feature_filter_type)\n",
    "one_hot_feature_names = ['parent_ner', 'child_ner', 'parent_pos', 'dependency_role','child_pos']\n",
    "one_hot_features = list(filtered_dependency_features_df.columns[filtered_dependency_features_df.columns.isin(one_hot_feature_names)])\n",
    "one_hot_encoder.fit(dependency_features_df[one_hot_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(zip(one_hot_feature_names, [len(i) for i in one_hot_encoder.categories_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_model = gensim.models.Word2Vec.load('id/id.bin.gz')\n",
    "word_vec = word_vec_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_train, word_vec, one_hot_encoder):\n",
    "    \n",
    "    word_feature_names = ['parent', 'child']\n",
    "    one_hot_feature_names = ['parent_ner', 'child_ner', 'parent_pos', 'dependency_role', 'child_pos']\n",
    "    \n",
    "    word_features = list(X_train.columns[X_train.columns.isin(word_feature_names)])\n",
    "    one_hot_features = list(X_train.columns[X_train.columns.isin(one_hot_feature_names)])\n",
    "    \n",
    "    def check_word_features(X):\n",
    "        columns = [column in word_features for column in X.columns]\n",
    "        return any(columns)\n",
    "\n",
    "    contains_word_features = check_word_features(X_train)\n",
    "    \n",
    "    X_train_dropped = X_train\n",
    "    if (contains_word_features):\n",
    "        X_train_dropped = X_train.drop(word_features+one_hot_features, axis=1)\n",
    "    else:\n",
    "        X_train_dropped = X_train.drop(one_hot_features, axis=1)\n",
    "    \n",
    "    if contains_word_features:\n",
    "        X_train_word_data = X_train[word_features]\n",
    "        # load word embedding\n",
    "        embeddings = []\n",
    "        for data in X_train_word_data.values:\n",
    "            current_embedding = []\n",
    "            for word in data:\n",
    "                splitted_word = word.split('_')[0]\n",
    "                if splitted_word in word_vec:\n",
    "                    current_embedding.append(word_vec[splitted_word])\n",
    "                else:\n",
    "                    none = [0] * 300\n",
    "                    current_embedding.append(none)\n",
    "            embeddings.append(current_embedding)\n",
    "    \n",
    "    # load one hot encoder\n",
    "    X_train_one_hot_data = X_train[one_hot_features]\n",
    "    X_train_one_hot_encoded = one_hot_encoder.transform(X_train_one_hot_data).toarray()\n",
    "    \n",
    "    X_train = []\n",
    "    for idx, row in enumerate(X_train_dropped.values):\n",
    "        concatenated = None\n",
    "        if (contains_word_features):\n",
    "            concatenated = np.concatenate([row, X_train_one_hot_encoded[idx], embeddings[idx][0],embeddings[idx][1]])\n",
    "        else:\n",
    "            concatenated = np.concatenate([row, X_train_one_hot_encoded[idx]])\n",
    "            \n",
    "        X_train.append(concatenated)\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_model(model_name, X_train, y_train, X_val, y_val, params):\n",
    "    supported_model_name = ['dtc', 'xgb']\n",
    "    if model_name not in supported_model_name:\n",
    "        raise Exception(\"Model not supported\")\n",
    "\n",
    "    if model_name == 'dtc':\n",
    "        model = DecisionTreeClassifier(random_state=RANDOM_STATE, **params)\n",
    "    elif model_name == 'xgb':\n",
    "        if not params:\n",
    "            params = {}\n",
    "        model = XGBClassifier(**params)\n",
    "    # Do training\n",
    "    model = model.fit(np.array(X_train), np.array(y_train))\n",
    "    # Do predict with val data\n",
    "    y_val_pred = model.predict(np.array(X_val))\n",
    "    \n",
    "    return model, y_val_pred\n",
    "\n",
    "def preprocess_features(X, y, one_hot_encoder, label_encoder):\n",
    "    # Preprocess features\n",
    "    X = preprocess(X, word_vec, one_hot_encoder)\n",
    "    # Preprocess labels\n",
    "    y = label_encoder.transform(np.ravel(y))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def run_hold_out_validation(dataset, split_percent, word_vec, label_encoder, one_hot_encoder,  model_name='dtc', params=None):\n",
    "    X_train, X_val, y_train, y_val = split_sentence(dataset, split_percent)\n",
    "    \n",
    "    # preprocess features\n",
    "    X_train, y_train = preprocess_features(X_train, y_train, one_hot_encoder, label_encoder)\n",
    "    X_val, y_val = preprocess_features(X_val, y_val, one_hot_encoder, label_encoder)\n",
    "\n",
    "    model, y_val_pred = fit_predict_model(model_name, X_train, y_train, X_val, y_val, params)\n",
    "    performance = get_metrics(y_val, y_val_pred, label_encoder)\n",
    "\n",
    "    return model, performance \n",
    "\n",
    "def build_amr_from_prediction(X,y):\n",
    "    predictions = create_pediction_graph(X, y)\n",
    "    amr_graphs = []\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        graph = create_amr_graph_from_prediction(prediction)\n",
    "        amr_graphs.append(graph)\n",
    "    return amr_graphs    \n",
    "\n",
    "def get_smatch_score(filepath):\n",
    "    smatch_output = !smatch.py -f {filepath}.pred.txt {filepath}.txt --significant 3\n",
    "    f_score = float(smatch_output[0].split(' ')[1])\n",
    "    return f_score\n",
    "\n",
    "def run_k_fold_validation(dataset, n_split, word_vec, label_encoder, one_hot_encoder, shuffle=False, feature_filter='all', model_name='dtc', params=None):\n",
    "    folds = custom_dependency_parser_feature_k_fold(dataset, n_split, shuffle=shuffle)\n",
    "    label = ['label']\n",
    "    \n",
    "    performances = defaultdict(list)\n",
    "    \n",
    "    for train_idx, val_idx in folds:\n",
    "        train_dataset, val_dataset = dataset.iloc[train_idx], dataset.iloc[val_idx]\n",
    "        X_train, y_train = train_dataset.drop(label, axis=1), train_dataset[label]\n",
    "        X_val, y_val = val_dataset.drop(label, axis=1), val_dataset[label]\n",
    "        \n",
    "        filtered_X_train = feature_selection(X_train, feature_filter)\n",
    "        filtered_X_val = feature_selection(X_val, feature_filter)\n",
    "        \n",
    "        # save AMR Data\n",
    "        amr_val_df = amr_dataset[amr_dataset.sentence_id.isin(X_val.sentence_id.unique())]\n",
    "        save_amr_data(amr_val_df, amr_val_prediction_path)\n",
    "        \n",
    "        # preprocess features\n",
    "        X_train_processed, y_train_processed = preprocess_features(filtered_X_train, y_train, one_hot_encoder, label_encoder)\n",
    "        X_val_processed, y_val_processed = preprocess_features(filtered_X_val, y_val, one_hot_encoder, label_encoder)\n",
    "        \n",
    "        model, y_val_pred = fit_predict_model(model_name, X_train_processed, y_train_processed, X_val_processed, y_val_processed, params)\n",
    "        performance = get_metrics(y_val_processed, y_val_pred, label_encoder)\n",
    "\n",
    "        # build prediction\n",
    "        amr_graphs = build_amr_from_prediction(X_val, y_val_pred)\n",
    "        save_amr_graphs(amr_graphs, amr_val_prediction_path)\n",
    "        f_score = get_smatch_score(amr_val_prediction_path)\n",
    "        \n",
    "        for key in performance:\n",
    "            performances[key].append(performance[key])\n",
    "        performances['smatch'].append(f_score)\n",
    "    \n",
    "    for key in performances:\n",
    "        performances[key] = np.mean(performances[key])\n",
    "    \n",
    "    return model, performances\n",
    "      \n",
    "def get_metrics(y_test, y_pred, label_encoder):\n",
    "    \n",
    "    y_true, y_pred = label_encoder.inverse_transform(y_test),label_encoder.inverse_transform(y_pred) \n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "#     precision_recall_fscore_support_score = precision_recall_fscore_support(y_true, y_pred)\n",
    "    \n",
    "    labels = label_encoder.classes_\n",
    "    cm = confusion_matrix(y_true, y_pred, labels)\n",
    "    \n",
    "    return dict(\n",
    "        f1_micro=f1_micro, \n",
    "        f1_macro=f1_macro, \n",
    "        accuracy=accuracy,\n",
    "#         precision_recall_fscore_support_score=precision_recall_fscore_support,\n",
    "#         cm=cm\n",
    "    )\n",
    "\n",
    "def print_performance_metrics(performance, labels):\n",
    "    accuracy = performance['accuracy']\n",
    "    f1_macro = performance['f1_macro']\n",
    "    f1_micro = performance['f1_micro']\n",
    "    smatch = performance.get('smatch', 0)\n",
    "    \n",
    "#     cm = performance['cm']\n",
    "    \n",
    "    print(\"Accuracy: {:.3f}\\nF1 Micro : {:.3f}\\nF1 Macro: {:.3f}\\nSMATCH: {:.3f}\".format(accuracy,f1_micro, f1_macro, smatch))\n",
    "#     print(performance['precision_recall_fscore_support_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pediction_graph(X_test, y_pred):\n",
    "    predictions = []\n",
    "#     X_test_val = X_test.values\n",
    "    cur_sentence_id = X_test['sentence_id'].values[0]\n",
    "    i = 0\n",
    "\n",
    "    while i < len(y_pred):\n",
    "        prediction = {\n",
    "            'nodes': [],\n",
    "            'heads': [],\n",
    "            'corefs': [],\n",
    "            'head_labels': [],\n",
    "            'sentence_id': 0\n",
    "        }\n",
    "        \n",
    "        current_pairs = []\n",
    "        root = None\n",
    "        # Collect nodes\n",
    "        while i < len(y_pred) and cur_sentence_id == X_test['sentence_id'].values[i]:\n",
    "            # add nodes\n",
    "            parent = X_test['parent'].values[i]\n",
    "            child = X_test['child'].values[i]\n",
    "            is_root = X_test['is_root'].values[i]\n",
    "            \n",
    "            if root is None and is_root==1:\n",
    "                root = parent\n",
    "            \n",
    "            if (parent not in prediction['nodes']):\n",
    "                prediction['nodes'].append(parent)\n",
    "            if (child not in prediction['nodes']):\n",
    "                prediction['nodes'].append(child)\n",
    "            current_pairs.append((parent, child, y_pred[i]))    \n",
    "            i+=1\n",
    "        \n",
    "        # Collect heads\n",
    "        for node in prediction['nodes']:\n",
    "            if node == root:\n",
    "                prediction['heads'].append(0)\n",
    "                prediction['head_labels'].append(':root')\n",
    "            else:\n",
    "                for pair in current_pairs:\n",
    "                    if (pair[1] == node):\n",
    "                        prediction['heads'].append(prediction['nodes'].index(pair[0])+1)\n",
    "                        prediction['head_labels'] += list(label_encoder.inverse_transform([pair[2]]))\n",
    "\n",
    "        prediction['corefs'] = list(range(1, len(prediction['nodes'])+1))\n",
    "        prediction['sentence_id'] = cur_sentence_id\n",
    "        prediction['root'] = root\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        if (i < len(y_pred)):\n",
    "            cur_sentence_id = X_test['sentence_id'].values[i]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def create_amr_graph_from_prediction(prediction):\n",
    "    nodes = prediction['nodes']\n",
    "    heads = prediction['heads']\n",
    "    head_labels = prediction['head_labels']\n",
    "    sentence_id = prediction['sentence_id']\n",
    "    root = prediction['root']\n",
    "    \n",
    "    \n",
    "    variable_map = dict()\n",
    "    triples = []\n",
    "    for idx, node in enumerate(nodes):\n",
    "        variable_map['vv'+str(idx+1)] = node\n",
    "    \n",
    "    # find top \n",
    "    top = 'vv1'\n",
    "    for var, value in variable_map.items():\n",
    "        if value == root:\n",
    "            top = var\n",
    "    \n",
    "    # rename nodes\n",
    "    for key in variable_map:\n",
    "        variable_map[key] = variable_map[key].split('_')[0]\n",
    "    \n",
    "    # create instances\n",
    "    for variable in variable_map:\n",
    "        triples.append((variable, 'instance', variable_map[variable]))\n",
    "\n",
    "    # create connections\n",
    "    for idx, head in enumerate(heads):\n",
    "        if (head != 0):\n",
    "            head_var = 'vv{}'.format(head)\n",
    "            dep_var = 'vv{}'.format(idx+1)\n",
    "            label = head_labels[idx]\n",
    "            triple = (head_var, label, dep_var)\n",
    "            triples.append(triple)\n",
    "        \n",
    "    graph = penman.Graph()\n",
    "    \n",
    "    graph.heads = heads\n",
    "    graph.nodes = nodes\n",
    "    graph.head_labels = head_labels\n",
    "    graph._top = top\n",
    "    graph._triples = [penman.Triple(*t) for t in triples]\n",
    "    graph.id = sentence_id\n",
    "    \n",
    "    return graph\n",
    "    \n",
    "def save_amr_graphs(amr_graphs, filepath):\n",
    "    with open(filepath + '.pred.txt', 'w+', encoding='utf-8') as f:\n",
    "        for idx, amr_graph in enumerate(amr_graphs):\n",
    "            try:\n",
    "                f.write(str(amr_graph))\n",
    "                f.write('\\n\\n')\n",
    "            except Exception as e:\n",
    "                display(amr_graph.id)\n",
    "                display(amr_graph._top)\n",
    "                display(amr_graph._triples)\n",
    "                display(amr_graph.nodes)                                                \n",
    "                display(amr_graph.heads)\n",
    "                display(amr_graph.head_labels)                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_report(experiments):\n",
    "    report_dict = defaultdict(list)\n",
    "    \n",
    "    for experiment in experiments:\n",
    "        report_dict['model_name'].append(experiment.name)\n",
    "        report_dict['accuracy'].append(experiment.performance['accuracy'])\n",
    "        report_dict['f1_macro'].append(experiment.performance['f1_macro'])\n",
    "        report_dict['smatch'].append(experiment.performance['smatch'])\n",
    "    \n",
    "    report_df = pd.DataFrame(report_dict)\n",
    "    report_df = report_df.set_index('model_name')\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModel:\n",
    "    def __init__(self, name, model, performance, labels):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.performance = performance\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __gt__(self, model2):\n",
    "        return self.performance['f1_macro'] > model2.performance['f1_macro']\n",
    "    \n",
    "    def print_metrics(self):\n",
    "        print(\"----{}----\".format(self.name))\n",
    "        print()\n",
    "        print_performance_metrics(self.performance, self.labels)\n",
    "        \n",
    "class ExperimentParams:\n",
    "    def __init__(self, model_name, model_params):\n",
    "        self.model_name = model_name\n",
    "        self.model_params = model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = []\n",
    "\n",
    "experiment_params.append(ExperimentParams(model_name='xgb', model_params=[\n",
    "#     {\"learning_rate\" : 0.05, 'max_depth': 5},\n",
    "#     {\"learning_rate\" : 0.05, 'max_depth': 8 },\n",
    "#     {\"learning_rate\" : 0.05, 'max_depth': 10},\n",
    "#     {\"learning_rate\" : 0.10, 'max_depth': 5 },\n",
    "    {\"learning_rate\" : 0.10, 'max_depth': 8},\n",
    "#     {\"learning_rate\" : 0.10, 'max_depth': 10},\n",
    "#     {\"learning_rate\" : 0.20, 'max_depth': 5},\n",
    "#     {\"learning_rate\" : 0.20, 'max_depth': 8},\n",
    "#     {\"learning_rate\" : 0.20, 'max_depth': 10},\n",
    "]))\n",
    "\n",
    "# experiment_params.append(ExperimentParams(model_name='dtc', model_params=[\n",
    "#     {'max_depth': 6 , 'criterion': 'entropy'},\n",
    "#     {'max_depth': 6 , 'criterion': 'gini'},\n",
    "#     {'max_depth': 7 , 'criterion': 'entropy'},\n",
    "#     {'max_depth': 7 , 'criterion': 'gini'}, \n",
    "#     {'max_depth': 10 , 'criterion': 'entropy'},\n",
    "#     {'max_depth': 10 , 'criterion': 'gini'},\n",
    "#     {'max_depth': 12 , 'criterion': 'entropy'},\n",
    "#     {'max_depth': 12 , 'criterion': 'gini'}\n",
    "# ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold Out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT_PERCENT = 0.20\n",
    "\n",
    "# trained_models = []\n",
    "# experiment_tqdm = tqdm(experiment_params)\n",
    "# for experiment_param in experiment_tqdm:\n",
    "#     for model_param in experiment_param.model_params: \n",
    "#         model, performance = run_hold_out_validation(dependency_features_df,\n",
    "#                                                    split_percent=SPLIT_PERCENT,\n",
    "#                                                    word_vec=word_vec,\n",
    "#                                                    label_encoder=label_encoder,\n",
    "#                                                    one_hot_encoder=one_hot_encoder,\n",
    "#                                                    model_name=experiment_param.model_name, \n",
    "#                                                    params=model_param)\n",
    "#         model_param_string = ''\n",
    "#         if (model_param is not None):\n",
    "#             model_param_string = '_'.join([\"{}_{}\".format(key, model_param[key]) for key in model_param])\n",
    "#         model_name = experiment_param.model_name + '_' + model_param_string\n",
    "        \n",
    "#         trained_models.append(TrainingModel(model_name, model, performance, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K_SPLIT = 5\n",
    "\n",
    "trained_models = []\n",
    "experiment_tqdm = tqdm(experiment_params)\n",
    "for experiment_param in experiment_tqdm:\n",
    "    train_tqdm = tqdm(experiment_param.model_params)\n",
    "    for model_param in train_tqdm: \n",
    "        model, performance = run_k_fold_validation(dependency_features_df,\n",
    "                                                   n_split=K_SPLIT,\n",
    "                                                   shuffle=True,\n",
    "                                                   word_vec=word_vec,\n",
    "                                                   label_encoder=label_encoder,\n",
    "                                                   one_hot_encoder=one_hot_encoder,\n",
    "                                                   feature_filter=feature_filter_type,\n",
    "                                                   model_name=experiment_param.model_name, \n",
    "                                                   params=model_param)\n",
    "        model_param_string = ''\n",
    "        if (model_param is not None):\n",
    "            model_param_string = '_'.join([\"{}_{}\".format(key, model_param[key]) for key in model_param])\n",
    "        model_name = experiment_param.model_name + '_' + model_param_string\n",
    "        trained_models.append(TrainingModel(model_name, model, performance, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_filter_type)\n",
    "report_df = create_experiment_report(trained_models)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = sorted(trained_models, reverse=True)[0]\n",
    "best_model.print_metrics()\n",
    "\n",
    "best_model_filename = \"saved_model/{}_best.pickle.dat\".format(best_model.name)\n",
    "with open(best_model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model.model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(best_model_filename, 'rb') as f:\n",
    "    best_model = pickle.load(open(best_model_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dependency_features_df.drop(['label'], axis=1)\n",
    "y = dependency_features_df['label']\n",
    "\n",
    "filtered_X = feature_selection(X, feature_filter_type)\n",
    "X_processed, y_processed = preprocess_features(filtered_X, y, one_hot_encoder, label_encoder)\n",
    "\n",
    "best_model = best_model.fit(np.array(X_processed), np.array(y_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model/best_model_pretrained.pickle.dat', 'wb') as f:\n",
    "    pickle.dump(best_model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save amr data\n",
    "save_amr_data(test_amr_dataset, amr_test_prediction_path)\n",
    "\n",
    "test_dependency_features_df = pd.read_csv(test_data_path)\n",
    "X_test, y_test = test_dependency_features_df.drop(['label'], axis=1), test_dependency_features_df['label']\n",
    "\n",
    "filtered_X_test = feature_selection(X_test, feature_filter_type)\n",
    "X_test_processed = preprocess(filtered_X_test, word_vec, one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(np.array(X_test_processed))\n",
    "performance = get_metrics(label_encoder.transform(y_test), y_test_pred, label_encoder)\n",
    "cm = confusion_matrix(label_encoder.transform(y_test), y_test_pred)\n",
    "\n",
    "print_performance_metrics(performance, label_encoder.classes_)\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, label_encoder.inverse_transform(y_test_pred), average=None, labels=label_encoder.classes_)\n",
    "print(label_encoder.classes_)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(fscore)\n",
    "print(support)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = create_pediction_graph(X_test, y_test_pred)\n",
    "amr_graphs = []\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    graph = create_amr_graph_from_prediction(prediction)\n",
    "    amr_graphs.append(graph)\n",
    "\n",
    "save_amr_graphs(amr_graphs, amr_test_prediction_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Test SMATCH score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!smatch.py -f data/test/test.pred.txt data/test/test.txt --pr --significant 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Berita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BERITA_DIR = \"data/raw/test/\"\n",
    "TEST_BERITA_FILE_NAMES = [\"b-salah-darat.csv\",\"c-gedung-roboh.csv\",\"d-indo-fuji.csv\", \"f-bunuh-diri.csv\", \"g-gempa-dieng.csv\"]\n",
    "TEST_BERITA_FILE_PATHS = [TEST_BERITA_DIR + \"labeled_df_\"+ filename for filename in TEST_BERITA_FILE_NAMES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in TEST_BERITA_FILE_NAMES:\n",
    "    topic = filepath.split('.')[0]\n",
    "    print(topic)\n",
    "    print()\n",
    "    \n",
    "    test_berita_filepath = TEST_BERITA_DIR+\"labeled_df_\"+filepath\n",
    "    test_dependency_features_df = pd.read_csv(test_berita_filepath)\n",
    "    X_test, y_test = test_dependency_features_df.drop(['label'], axis=1), test_dependency_features_df['label']\n",
    "\n",
    "    filtered_X_test = feature_selection(X_test, feature_filter_type)\n",
    "    X_test_processed = preprocess(filtered_X_test, word_vec, one_hot_encoder)\n",
    "    \n",
    "    y_test_pred = best_model.predict(np.array(X_test_processed))\n",
    "    performance = get_metrics(label_encoder.transform(y_test), y_test_pred, label_encoder)\n",
    "    cm = confusion_matrix(label_encoder.transform(y_test), y_test_pred)\n",
    "\n",
    "    print_performance_metrics(performance, label_encoder.classes_)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, label_encoder.inverse_transform(y_test_pred), average=None, labels=label_encoder.classes_)\n",
    "    print(label_encoder.classes_)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(fscore)\n",
    "    print(support)\n",
    "\n",
    "#     ax= plt.subplot()\n",
    "#     sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "#     # labels, title and ticks\n",
    "#     ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "#     ax.set_title('Confusion Matrix'); \n",
    "#     ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n",
    "#     plt.show()\n",
    "    \n",
    "    predictions = create_pediction_graph(X_test, y_test_pred)\n",
    "    amr_graphs = []\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        graph = create_amr_graph_from_prediction(prediction)\n",
    "        amr_graphs.append(graph)\n",
    "\n",
    "    amr_output_path = TEST_BERITA_DIR+'amr_output_'+ topic\n",
    "    save_amr_graphs(amr_graphs, amr_output_path)\n",
    "    amr_test_path = TEST_BERITA_DIR+topic+\".txt\"\n",
    "    \n",
    "    !smatch.py -f {amr_output_path}.pred.txt {amr_test_path} --pr --significant 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
