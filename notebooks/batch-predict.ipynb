{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\MasterDegree\\AMR\\amr-parser\n"
     ]
    }
   ],
   "source": [
    "# Make sure our working directory is the root of project, e.g /amr-parser\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'penman' from 'D:\\\\miniconda3\\\\envs\\\\amr\\\\lib\\\\site-packages\\\\penman.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "\n",
    "import nltk\n",
    "import penman\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "reload(penman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-21 08:53:16,124 INFO] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2021-12-21 08:53:16 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "[2021-12-21 08:53:16,179 INFO] Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-12-21 08:53:16 INFO: Use device: cpu\n",
      "[2021-12-21 08:53:16,179 INFO] Use device: cpu\n",
      "2021-12-21 08:53:16 INFO: Loading: tokenize\n",
      "[2021-12-21 08:53:16,189 INFO] Loading: tokenize\n",
      "2021-12-21 08:53:16 INFO: Loading: pos\n",
      "[2021-12-21 08:53:16,229 INFO] Loading: pos\n",
      "2021-12-21 08:53:16 INFO: Loading: lemma\n",
      "[2021-12-21 08:53:16,722 INFO] Loading: lemma\n",
      "2021-12-21 08:53:16 INFO: Loading: depparse\n",
      "[2021-12-21 08:53:16,769 INFO] Loading: depparse\n",
      "2021-12-21 08:53:17 INFO: Done loading processors!\n",
      "[2021-12-21 08:53:17,294 INFO] Done loading processors!\n",
      "[2021-12-21 08:53:17,730 INFO] loading Word2Vec object from pretrained/word2vec/id/id.bin.gz\n",
      "[2021-12-21 08:53:18,387 INFO] setting ignored attribute syn0norm to None\n",
      "[2021-12-21 08:53:18,387 INFO] setting ignored attribute cum_table to None\n",
      "[2021-12-21 08:53:18,387 INFO] Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.\n",
      "[2021-12-21 08:53:18,387 INFO] loading Word2Vec object from pretrained/word2vec/id/id.bin.gz\n",
      "[2021-12-21 08:53:19,256 INFO] setting ignored attribute syn0norm to None\n",
      "[2021-12-21 08:53:19,256 INFO] setting ignored attribute cum_table to None\n",
      "[2021-12-21 08:53:19,256 INFO] loaded pretrained/word2vec/id/id.bin.gz\n",
      "D:\\miniconda3\\envs\\amr\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\miniconda3\\envs\\amr\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from prediction.predict import predict_and_process, process_sentence\n",
    "\n",
    "# from utils.amr import penman_to_dot\n",
    "from utils.ner.entity_recognizer import get_entities, get_entities_tf\n",
    "from utils.amr_parsing.io import AMRIO\n",
    "from utils.amr_parsing.amr import AMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True) #kalau cudablas nya ga mau, run ini sebelum import get_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAnnotator:\n",
    "    word_dict = {}\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.nlp = stanza.Pipeline(lang=\"id\",use_gpu=True,verbose=False, tokenize_pretokenized=True)\n",
    "        self.annotation={}\n",
    "        factory = StemmerFactory()\n",
    "        self.stemmer = factory.create_stemmer()\n",
    "        self.params = params\n",
    "        if self.params[\"ner_tagger\"]=='tf' :\n",
    "            self.ner = get_entities_tf\n",
    "        else:\n",
    "            self.ner = get_entities\n",
    "        \n",
    "        if self.params['pos_tagger'] == 'nltk':\n",
    "            self.pos_tagger = nltk.tag.CRFTagger()\n",
    "            self.pos_tagger.set_model_file('pretrained/pos_tagger/all_indo_man_tag_corpus_model.crf.tagger')\n",
    "        elif self.params['pos_tagger'] == 'stanza':\n",
    "            pass\n",
    "        \n",
    "    def annotate(self, sentence):\n",
    "        self.annotation = defaultdict(list)\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        self.annotation[\"sentence\"] = sentence\n",
    "        doc = self.nlp(sentence)\n",
    "        \n",
    "        self.annotation['ner_tags'] = self.ner(sentence)\n",
    "        \n",
    "        word_dict = defaultdict(int)\n",
    "        \n",
    "        for sent in doc.sentences:\n",
    "            for idx, word in enumerate(sent.words):\n",
    "                self.annotation['tokens'].append(word.text)\n",
    "                stemmed_word = self.stemmer.stem(word.text)                \n",
    "                if (self.annotation['ner_tags'][idx] in ['PER', 'ORG']):\n",
    "                    stemmed_word = word.text.lower()\n",
    "                word_dict[stemmed_word] += 1\n",
    "                self.annotation['lemmas'].append(stemmed_word)\n",
    "                self.annotation['pos_tags'].append(word.upos)\n",
    "    \n",
    "        if self.params['pos_tagger'] == 'nltk':\n",
    "            self.annotation['pos_tags'] = [tag[1] for tag in self.pos_tagger.tag(self.annotation['tokens'])]\n",
    "            \n",
    "        \n",
    "        return self.annotation\n",
    "\n",
    "ner_labels_map = {\n",
    "    \"PER\" : \"PERSON\",\n",
    "    \"LOC\" : \"LOCATION\",\n",
    "    \"FAC\" : \"LOCATION\",\n",
    "    \"GPE\" : \"LOCATION\",\n",
    "    \"NOR\" : \"ORGANIZATION\",\n",
    "    \"ORG\" : \"ORGANIZATION\",\n",
    "    \"MON\" : \"MONEY\",\n",
    "    \"CRD\" : \"NUMBER\",\n",
    "    \"ORD\" : \"ORDINAL\",\n",
    "    \"PRC\" : \"PERCENT\",\n",
    "    \"DAT\" : \"DATE\",\n",
    "    \"TIM\" : \"TIME\",\n",
    "}\n",
    "\n",
    "def transform_ner_tags(annotation):\n",
    "    ner_tags = annotation[\"ner_tags\"]\n",
    "    for i, ner in enumerate(ner_tags):\n",
    "        annotation[\"ner_tags\"][i] = ner_labels_map.get(ner.split(\"-\")[-1],\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_amr_features(amr_graph, annotation, f, index=None):\n",
    "    amr = AMR()\n",
    "    if amr_graph != \"\":\n",
    "        amr.graph = amr_graph\n",
    "    if index:\n",
    "        amr.id = index\n",
    "    amr.sentence = annotation[\"sentence\"].strip()\n",
    "    amr.tokens = annotation['tokens']\n",
    "    amr.lemmas = annotation['lemmas']\n",
    "    amr.pos_tags = annotation['pos_tags']\n",
    "    amr.ner_tags = annotation['ner_tags']\n",
    "    AMRIO.dump([amr], f)\n",
    "\n",
    "def predict_sentence(sentence,annotator, f, index=0):\n",
    "    try:\n",
    "        annotation = annotator.annotate(sentence)\n",
    "    except Exception as e:\n",
    "        print(f\"Problem with sentence : {sentence}\")\n",
    "        return\n",
    "    try:\n",
    "        filtered_X = process_sentence(sentence)\n",
    "        amr_graph = predict_and_process(filtered_X)\n",
    "        dump_amr_features(amr_graph, annotation, f, index)\n",
    "    except Exception as e:\n",
    "        print(\"problem with\", e)\n",
    "        print(annotation)\n",
    "        dump_amr_features(\"\", annotation, f, index)\n",
    "\n",
    "def prepare_sentence_for_parsing(filepath, params, start=1):\n",
    "    index = 0\n",
    "    annotator = FeatureAnnotator(params)\n",
    "    outfile = filepath if start == 1 else filepath+f\".{str(start)}\"\n",
    "    with open(outfile + '.features', 'w', encoding='utf-8') as f:\n",
    "        with open(filepath, 'r', encoding='utf-8') as t:\n",
    "            for sentence in tqdm(t):\n",
    "                index += 1\n",
    "                if index < start:\n",
    "                    continue\n",
    "                try:\n",
    "                    annotation = annotator.annotate(sentence)\n",
    "                except Exception as e:\n",
    "                    print(f\"Problem with sentence : {sentence}\")\n",
    "                    return\n",
    "                dump_amr_features(\"(t/temp)\", annotation, f, index)\n",
    "\n",
    "def preprocess_sentence(filepath, params):\n",
    "    index = 0\n",
    "    annotator = FeatureAnnotator(params)\n",
    "    with open(filepath + '.features.pred', 'w', encoding='utf-8') as f:\n",
    "        with open(filepath, 'r', encoding='utf-8') as t:\n",
    "            for sentence in tqdm(t):\n",
    "                index += 1\n",
    "                predict_sentence(sentence,annotator,f, index=index)\n",
    "\n",
    "def predict_from_amr(filepath, params):\n",
    "    annotator = FeatureAnnotator(params)\n",
    "    \n",
    "    with open(filepath + '.features.pred', 'w', encoding='utf-8') as f:\n",
    "        for i, amr in enumerate(AMRIO.read(filepath), 1):\n",
    "            if i % 100 == 0:\n",
    "                print('{} processed.'.format(i))\n",
    "                \n",
    "            predict_sentence(amr.sentence, annotator, f, amr.id)\n",
    "\n",
    "def preprocess_amr(filepath, params):\n",
    "    annotator = FeatureAnnotator(params)\n",
    "    \n",
    "    with open(filepath + '.features', 'w', encoding='utf-8') as f:\n",
    "        i = 1\n",
    "        for amr in AMRIO.read(filepath):\n",
    "            if i % 100 == 0:\n",
    "                print('{} processed.'.format(i))\n",
    "            i+=1\n",
    "            annotation = annotator.annotate(amr.sentence)\n",
    "            transform_ner_tags(annotation)\n",
    "            dump_amr_features(amr.graph, annotation, f, index=amr.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator=FeatureAnnotator({'pos_tagger': 'nltk', 'ner_tagger':'anago'})\n",
    "with open(\"data/amr_experiment/test_1.txt\", \"w\") as f:\n",
    "    predict_sentence(\"Aku membuang Roti itu ke tempat sampah\", annotator, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21839it [20:09, 18.06it/s]  \n"
     ]
    }
   ],
   "source": [
    "# prepare_sentence_for_parsing(\"data/bppt_id_en/split/dev_id.txt\", {'pos_tagger': 'nltk', 'ner_tagger':'anago'})\n",
    "prepare_sentence_for_parsing(\"data/bppt_id_en/split/train_id.txt\", {'pos_tagger': 'nltk', 'ner_tagger':'anago'}, start=19214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 processed.\n",
      "200 processed.\n",
      "300 processed.\n",
      "400 processed.\n",
      "500 processed.\n",
      "600 processed.\n",
      "700 processed.\n",
      "800 processed.\n",
      "900 processed.\n",
      "1000 processed.\n",
      "1100 processed.\n",
      "1200 processed.\n",
      "1300 processed.\n",
      "1400 processed.\n",
      "1500 processed.\n",
      "1600 processed.\n",
      "1700 processed.\n",
      "1800 processed.\n",
      "1900 processed.\n",
      "2000 processed.\n",
      "2100 processed.\n",
      "2200 processed.\n",
      "2300 processed.\n",
      "Exception raised : \n",
      " (d / dan    :op1 (j / jadi            :ARG0 (h / hendra)            :ARG1 (a / alumni                     :mod (l / lulusan                             :mod (b / baik)))            :location (u / universitas                         :mod (i / islam                                 :location (m / malang)))            :time (t / tahun                     :mod (_ / 2013)))    :op2 (d2 / dagang             :ARG0 (a2 / ayah)             :ARG1 (s / siomay))) 358 (\n",
      "(d / dan    :op1 (j / jadi            :ARG0 (h / hendra)            :ARG1 (a / alumni                     :mod (l / lulusan                             :mod (b / baik)))            :location (u / universitas                         :mod (i / islam                                 :location (m / malang)))            :time (t / tahun                     :mod (_ / 2013)))    :op2 (d2 / dagang             :ARG0 (a2 / ayah)             :ARG1 (s / siomay)))\n",
      "Expected \"(\", but found  (_ / 2013) from patter \\s*(\\()\\s*(?=[a-z]+\\d*\\s*\\/) at position 358\n",
      "ERR_START\n",
      "['(d / dan', '   :op1 (j / jadi', '           :ARG0 (h / hendra)', '           :ARG1 (a / alumni', '                    :mod (l / lulusan', '                            :mod (b / baik)))', '           :location (u / universitas', '                        :mod (i / islam', '                                :location (m / malang)))', '           :time (t / tahun', '                    :mod (_ / 2013)))', '   :op2 (d2 / dagang', '            :ARG0 (a2 / ayah)', '            :ARG1 (s / siomay)))']\n",
      "'NoneType' object has no attribute 'set_src_tokens'\n",
      "Exception raised : \n",
      " (d / dan    :op1 (a / ajar            :ARG0 (a2 / ayah)            :location (s / sekolah                         :mod (d2 / dasar                                  :mod (n / negeri))                         :name (t / tamanan                                  :mod (_ / 1)))            :time (t2 / tahun))    :op2 (m / main            :ARG0 (a3 / adik)            :ARG1 (b / boneka                     :name (b2 / barbie)))) 264 (\n",
      "(d / dan    :op1 (a / ajar            :ARG0 (a2 / ayah)            :location (s / sekolah                         :mod (d2 / dasar                                  :mod (n / negeri))                         :name (t / tamanan                                  :mod (_ / 1)))            :time (t2 / tahun))    :op2 (m / main            :ARG0 (a3 / adik)            :ARG1 (b / boneka                     :name (b2 / barbie))))\n",
      "Expected \"(\", but found  (_ / 1)))  from patter \\s*(\\()\\s*(?=[a-z]+\\d*\\s*\\/) at position 264\n",
      "ERR_START\n",
      "['(d / dan', '   :op1 (a / ajar', '           :ARG0 (a2 / ayah)', '           :location (s / sekolah', '                        :mod (d2 / dasar', '                                 :mod (n / negeri))', '                        :name (t / tamanan', '                                 :mod (_ / 1)))', '           :time (t2 / tahun))', '   :op2 (m / main', '           :ARG0 (a3 / adik)', '           :ARG1 (b / boneka', '                    :name (b2 / barbie))))']\n",
      "'NoneType' object has no attribute 'set_src_tokens'\n",
      "2400 processed.\n",
      "2500 processed.\n",
      "2600 processed.\n",
      "Exception raised : \n",
      " (a / atau    :op1 (j / jadi            :ARG0 (h / hendra)            :ARG1 (a2 / alumni                      :mod (l / lulusan                              :mod (b / baik)))            :location (u / universitas                         :mod (i / islam                                 :location (m / malang)))            :time (t / tahun                     :mod (_ / 2013)))    :op2 (d / dagang            :ARG0 (a3 / ayah)            :ARG1 (s / siomay))) 362 (\n",
      "(a / atau    :op1 (j / jadi            :ARG0 (h / hendra)            :ARG1 (a2 / alumni                      :mod (l / lulusan                              :mod (b / baik)))            :location (u / universitas                         :mod (i / islam                                 :location (m / malang)))            :time (t / tahun                     :mod (_ / 2013)))    :op2 (d / dagang            :ARG0 (a3 / ayah)            :ARG1 (s / siomay)))\n",
      "Expected \"(\", but found  (_ / 2013) from patter \\s*(\\()\\s*(?=[a-z]+\\d*\\s*\\/) at position 362\n",
      "ERR_START\n",
      "['(a / atau', '   :op1 (j / jadi', '           :ARG0 (h / hendra)', '           :ARG1 (a2 / alumni', '                     :mod (l / lulusan', '                             :mod (b / baik)))', '           :location (u / universitas', '                        :mod (i / islam', '                                :location (m / malang)))', '           :time (t / tahun', '                    :mod (_ / 2013)))', '   :op2 (d / dagang', '           :ARG0 (a3 / ayah)', '           :ARG1 (s / siomay)))']\n",
      "'NoneType' object has no attribute 'set_src_tokens'\n",
      "Exception raised : \n",
      " (a / atau    :op1 (a2 / ajar             :ARG0 (a3 / ayah)             :location (s / sekolah                          :mod (d / dasar                                  :mod (n / negeri))                          :name (t / tamanan                                   :mod (_ / 1)))             :time (t2 / tahun))    :op2 (m / main            :ARG0 (a4 / adik)            :ARG1 (b / boneka                     :name (b2 / barbie)))) 270 (\n",
      "(a / atau    :op1 (a2 / ajar             :ARG0 (a3 / ayah)             :location (s / sekolah                          :mod (d / dasar                                  :mod (n / negeri))                          :name (t / tamanan                                   :mod (_ / 1)))             :time (t2 / tahun))    :op2 (m / main            :ARG0 (a4 / adik)            :ARG1 (b / boneka                     :name (b2 / barbie))))\n",
      "Expected \"(\", but found  (_ / 1)))  from patter \\s*(\\()\\s*(?=[a-z]+\\d*\\s*\\/) at position 270\n",
      "ERR_START\n",
      "['(a / atau', '   :op1 (a2 / ajar', '            :ARG0 (a3 / ayah)', '            :location (s / sekolah', '                         :mod (d / dasar', '                                 :mod (n / negeri))', '                         :name (t / tamanan', '                                  :mod (_ / 1)))', '            :time (t2 / tahun))', '   :op2 (m / main', '           :ARG0 (a4 / adik)', '           :ARG1 (b / boneka', '                    :name (b2 / barbie))))']\n",
      "'NoneType' object has no attribute 'set_src_tokens'\n",
      "100 processed.\n",
      "200 processed.\n",
      "300 processed.\n"
     ]
    }
   ],
   "source": [
    "preprocess_amr(\"data/amr_id_2.0/train.txt\", {'pos_tagger': 'nltk', 'ner_tagger':'anago'})\n",
    "preprocess_amr(\"data/amr_id_2.0/dev.txt\", {'pos_tagger': 'nltk', 'ner_tagger':'anago'})\n",
    "preprocess_amr(\"data/amr_id_2.0/test.txt\", {'pos_tagger': 'nltk', 'ner_tagger':'anago'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Yasmin membuang sampah di tempat sampah\"\n",
    "annotator = FeatureAnnotator({'pos_tagger': 'nltk', 'ner_tagger':'tf'})\n",
    "annotation = annotator.annotate(sentence)\n",
    "filtered_X = process_sentence(sentence)\n",
    "amr_graph = predict_and_process(filtered_X)\n",
    "# amr_graph = predict_and_process(filtered_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amr_graph._triples.pop(3) \n",
    "# amr_graph.triples()\n",
    "with open(\"error.amr\", 'w', encoding='utf-8') as f:\n",
    "    dump_amr_features(amr_graph, annotation, f, 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ca5d393f6b226cd3ca7fbdd8acf7d8339a1fda0aa6b601d6b6cc21d35a38bcb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('amr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
